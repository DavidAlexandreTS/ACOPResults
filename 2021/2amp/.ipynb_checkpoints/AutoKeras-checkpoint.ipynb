{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import copy \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import autokeras as ak\n",
    "import tensorflow as tf\n",
    "\n",
    "models   = []\n",
    "graphs   = []\n",
    "sessions = []\n",
    "\n",
    "strategyArray = []\n",
    "gainArray     = []\n",
    "lossArray     = []\n",
    "ampNumber     = 2\n",
    "\n",
    "originalDataSet = pd.read_csv('dataset.txt', sep = ',',header = None)\n",
    "originalDataSet = originalDataSet.sample(frac = 1, random_state = 5)\n",
    "binaryDataSet   = []\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "for line in originalDataSet.values:\n",
    "    myList = [1 if i != 0 else 0 for i in line[:40]]\n",
    "    myList.extend(line[40:])\n",
    "    binaryDataSet.append(myList)\n",
    "\n",
    "binaryDataSet = pd.DataFrame(binaryDataSet)\n",
    "gainScaler    = MinMaxScaler(feature_range = (-1, 1))\n",
    "lossScaler    = MinMaxScaler(feature_range = (-1, 1))\n",
    "\n",
    "gainScalerAux = []\n",
    "lossScalerAux = []\n",
    "\n",
    "for i in range(0, ampNumber * 2, 2):\n",
    "    gainScalerAux.extend(binaryDataSet.values[:, 40 + i])\n",
    "    lossScalerAux.extend(binaryDataSet.values[:, 40 + i + 1])\n",
    "\n",
    "gainScaler.fit(np.array(gainScalerAux).reshape(-1, 1))\n",
    "lossScaler.fit(np.array(lossScalerAux).reshape(-1, 1))\n",
    "\n",
    "def loadDataset(): \n",
    "    dataSet = binaryDataSet.values[:, :40]\n",
    "    \n",
    "    for i in range(0, ampNumber * 2, 2):\n",
    "        gain    = np.array(binaryDataSet.values[:, 40 + i])\n",
    "        gain    = gainScaler.transform(gain.reshape(-1, 1))\n",
    "        #gain    = gain.reshape(-1, 1)\n",
    "        dataSet = np.hstack((dataSet, gain))\n",
    "        \n",
    "        loss    = np.array(binaryDataSet.values[:, 40 + i + 1])\n",
    "        loss    = lossScaler.transform(loss.reshape(-1, 1))\n",
    "        #loss    = loss.reshape(-1, 1)\n",
    "        dataSet = np.hstack((dataSet, loss))\n",
    "            \n",
    "    X, y = np.array(dataSet[:, :40]), np.array(dataSet[:, 40:])\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def invertGainNorm(value):\n",
    "    auxArray = np.array([value, 0, 0, 0, 0, 0]).reshape(-1, 1)\n",
    "    return round(gainScaler.inverse_transform(auxArray)[0][0])\n",
    "\n",
    "def invertLossNorm(value):\n",
    "    auxArray = np.array([value, 0, 0, 0, 0, 0]).reshape(-1, 1)\n",
    "    return round(lossScaler.inverse_transform(auxArray)[0][0])\n",
    "    \n",
    "def getGainError(value1, value2):\n",
    "    return abs(invertGainNorm(value1) - invertGainNorm(value2))\n",
    "\n",
    "def getLossError(value1, value2):\n",
    "    return abs(invertLossNorm(value1) - invertLossNorm(value2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initModels():\n",
    "    models   = []\n",
    "    graphs   = []\n",
    "    sessions = []\n",
    "    \n",
    "    for i in range(ampNumber):        \n",
    "        graphs.append(tf.Graph())\n",
    "        session.append(tf.compat.v1.Session(graph = graph))\n",
    "\n",
    "    for i in range(ampNumber):\n",
    "        with graph.as_default():\n",
    "            with session.as_default():\n",
    "                models.append(ak.StructuredDataRegressor(max_trials=3, overwrite=True))\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(regressor, X, y, folds = 5):\n",
    "    foldSize   = math.ceil(X.shape[0] / folds)\n",
    "    gainErrors = []\n",
    "    lossErrors = []\n",
    "    \n",
    "    for i in range(folds): \n",
    "        sliceBegin = i * foldSize\n",
    "        sliceEnd   = (i + 1) * foldSize\n",
    "        \n",
    "        X_train = np.delete(X, np.s_[sliceBegin: sliceEnd], 0)\n",
    "        y_train = np.delete(y, np.s_[sliceBegin: sliceEnd], 0)\n",
    "        \n",
    "        regressor.fit(X_train, y_train, verbose = False)\n",
    "        X_test = X[sliceBegin: sliceEnd]\n",
    "        y_test = y[sliceBegin: sliceEnd]\n",
    "        \n",
    "        gainError = 0\n",
    "        lossError = 0\n",
    "        \n",
    "        prediction = regressor.predict(X_test)\n",
    "        \n",
    "        for predicted, expected in zip(prediction, y_test):\n",
    "            gainError += getGainError(predicted[0], expected[0]) \n",
    "            lossError += getLossError(predicted[1], expected[1])\n",
    "                 \n",
    "        gainErrors.append((gainError / ampNumber) / foldSize)\n",
    "        lossErrors.append((lossError / ampNumber) / foldSize) # average loss error by amp\n",
    "        \n",
    "    return np.array(gainErrors), np.array(lossErrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trainModel(models):   \n",
    "    models.append(ak.StructuredDataRegressor(max_trials=3, overwrite=True))\n",
    "    \n",
    "    X, y                   = loadDataset()\n",
    "    gainErrors, lossErrors = crossValidate(models[0], X, y[:, 0: 2])\n",
    "    print(gainErrors, \"=> %0.2f (+/- %0.2f)\" % (np.mean(gainErrors), gainErrors.std() * 2))\n",
    "    print(lossErrors, \"=> %0.2f (+/- %0.2f)\\n\" % (np.mean(lossErrors), lossErrors.std() * 2))\n",
    "    \n",
    "    gainArray.append(gainErrors)\n",
    "    lossArray.append(lossErrors)\n",
    "    strategyArray.append(\"ANN - Amp 1\")\n",
    "    \n",
    "    prediction = models[0].predict(X)\n",
    "    \n",
    "    for i in range(1, ampNumber):\n",
    "        models.append(ak.StructuredDataRegressor(max_trials=3, overwrite=True))\n",
    "        #gainErrors, lossErrors = crossValidate(models[i], X, y[:, i * 2: i * 2 + 2]) \n",
    "        gainErrors, lossErrors = crossValidate(models[i], np.hstack((X, prediction)), y[:, i * 2: i * 2 + 2])  \n",
    "        print(gainErrors, \"=> %0.2f (+/- %0.2f)\" % (np.mean(gainErrors), gainErrors.std() * 2))\n",
    "        print(lossErrors, \"=> %0.2f (+/- %0.2f)\\n\" % (np.mean(lossErrors), lossErrors.std() * 2))\n",
    "\n",
    "        gainArray.append(gainErrors)\n",
    "        lossArray.append(lossErrors)\n",
    "        strategyArray.append(\"ANN - Amp {}\".format(i + 1))\n",
    "        \n",
    "        prediction = models[i].predict(np.hstack((X, prediction)))\n",
    "        #prediction = models[i].predict(X)\n",
    "    \n",
    "    return gainErrors, lossErrors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDistribution(models):\n",
    "    train_results = []\n",
    "    test_results  = []\n",
    "    X, y                             = loadDataset()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 5)\n",
    "    \n",
    "    models[0].fit(X_train, y_train[:, 0: 2])\n",
    "    \n",
    "    train_results.append(models[0].predict(X_train))\n",
    "    test_results.append(models[0].predict(X_test))\n",
    "    \n",
    "    for i in range(1, ampNumber):\n",
    "        models[i].fit(np.hstack((X_train, train_results[i - 1])), y_train[:, i * 2: i * 2 + 2])\n",
    "        train_results.append(models[i].predict(np.hstack((X_train, train_results[i - 1]))))\n",
    "        test_results.append(models[i].predict(np.hstack((X_test, test_results[i - 1]))))\n",
    "    \n",
    "    fig   = plt.figure(figsize = (15, 5))\n",
    "    vGain = np.vectorize(invertGainNorm)\n",
    "    vLoss = np.vectorize(invertLossNorm)\n",
    "    \n",
    "    for i in range(ampNumber):\n",
    "        ax = fig.add_subplot(2, 2, i + 1)\n",
    "        \n",
    "        yGain    = vGain(y_test[:, i * 2])\n",
    "        yLoss    = vLoss(y_test[:, i * 2 + 1])\n",
    "        testGain = vGain(test_results[i][:, 0])\n",
    "        testLoss = vLoss(test_results[i][:, 1])\n",
    "        \n",
    "        ax.scatter(yGain, yLoss)\n",
    "        ax.scatter(testGain, testLoss, c = \"m\")\n",
    "        ax.set_title(\"Amplifier {}\".format(i + 1))\n",
    "    \n",
    "    fig.suptitle(\"Test Error\", fontsize = 16)\n",
    "    fig.text(0.5, 0.04, 'Gain', ha='center', va='center', fontsize = 14)\n",
    "    fig.text(0.06, 0.5, 'Loss', ha='center', va='center', rotation='vertical', fontsize = 14)\n",
    "    fig.legend([\"expected\", \"predicted\"])\n",
    "\n",
    "    plt.show()\n",
    "    return y_test, test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGainError(yTest, predicted):\n",
    "    fig, axis = plt.subplots(figsize = (10, 5))\n",
    "    data      = []\n",
    "    \n",
    "    for i in range(ampNumber):\n",
    "        gainError = []\n",
    "        \n",
    "        yTestAmp     = yTest[:, i * 2]\n",
    "        predictedAmp = predicted[i][:, 0]\n",
    "        \n",
    "        for a, b in zip(predictedAmp, yTestAmp):\n",
    "            gainError.append(getGainError(a, b)) \n",
    "        \n",
    "        data.append(gainError)\n",
    "        \n",
    "    axis.boxplot(data)\n",
    "    axis.set_title(\"Test Gain Error\", fontsize = 16)\n",
    "    axis.set_xticklabels(np.repeat(strategyArray, 1))\n",
    "    axis.set_ylabel(\"Absolute error (dB)\", fontsize = 14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossError(yTest, predicted):\n",
    "    fig, axis = plt.subplots(figsize = (10, 5))\n",
    "    data      = []\n",
    "    \n",
    "    for i in range(ampNumber):\n",
    "        lossError = []\n",
    "        \n",
    "        yTestAmp     = yTest[:, i * 2 + 1]\n",
    "        predictedAmp = predicted[i][:, 1]\n",
    "        \n",
    "        for a, b in zip(predictedAmp, yTestAmp):\n",
    "            lossError.append(getLossError(a, b)) \n",
    "        \n",
    "        data.append(lossError)\n",
    "        \n",
    "    axis.boxplot(data)\n",
    "    axis.set_title(\"Test Loss Error\", fontsize = 16)\n",
    "    axis.set_xticklabels(np.repeat(strategyArray, 1))\n",
    "    axis.set_ylabel(\"Absolute error (dB)\", fontsize = 14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLinkTestError(yTest, predicted):\n",
    "    fig, axis = plt.subplots(figsize = (10, 5))\n",
    "    lossData  = [] \n",
    "    gainData  = [] \n",
    "    \n",
    "    yTestGain     = yTest[:, 0]\n",
    "    yTestLoss     = yTest[:, 1]\n",
    "    predictedGain = np.array(predicted[0][:, 0]).flatten()\n",
    "    predictedLoss = np.array(predicted[0][:, 1]).flatten()\n",
    "    \n",
    "    for i in range(1, ampNumber):\n",
    "        yTestGain     = np.hstack((yTestGain, yTest[:, i * 2]))\n",
    "        yTestLoss     = np.hstack((yTestLoss, yTest[:, i * 2 + 1]))\n",
    "        predictedGain = np.hstack((predictedGain, np.array(predicted[i][:, 0]).flatten()))\n",
    "        predictedLoss = np.hstack((predictedLoss, np.array(predicted[i][:, 1]).flatten()))\n",
    "    \n",
    "    for i in range(len(yTestGain)):\n",
    "        lossData.append(getLossError(predictedLoss[i], yTestLoss[i]))\n",
    "        gainData.append(getGainError(predictedGain[i], yTestGain[i]))\n",
    "            \n",
    "    print(\"Loss error mean: {}. Loss error median: {}\".format(np.mean(lossData), np.median(lossData)))\n",
    "    print(\"Gain error mean: {}. Gain error median: {}\".format(np.mean(gainData), np.median(gainData)))\n",
    "    \n",
    "    axis.boxplot([lossData, gainData])\n",
    "    axis.set_title(\"Link Test Error\", fontsize = 16)\n",
    "    axis.set_xticklabels([\"Gain\", \"Loss\"])\n",
    "    axis.set_ylabel(\"Absolute error (dB)\", fontsize = 14)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearningCurves(ampNumber, model, X, y, cv):\n",
    "    fig, axis                              = plt.subplots(figsize = (10, 5))\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv = cv, \n",
    "                                                            train_sizes = np.linspace(0.1, 1.0, 6),\n",
    "                                                            scoring = \"neg_mean_squared_error\")\n",
    "       \n",
    "    train_scores = -train_scores\n",
    "    test_scores  = -test_scores\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis = 1)\n",
    "    train_scores_std  = np.std(train_scores, axis = 1)\n",
    "    test_scores_mean  = np.mean(test_scores, axis = 1)\n",
    "    test_scores_std   = np.std(test_scores, axis = 1)\n",
    "    \n",
    "    print(\"Size splits: {}\".format(train_sizes))\n",
    "    print(\"Train scores: {} \".format(train_scores_mean))\n",
    "    print(\"Test scores: {} \".format(test_scores_mean))\n",
    "\n",
    "    # Plot learning curve\n",
    "    axis.plot(train_sizes, train_scores_mean, 'o-', color = \"r\", label = \"Training score\")\n",
    "    axis.plot(train_sizes, test_scores_mean, 'o-', color = \"g\", label = \"Test score\")\n",
    "    axis.set_title(\"Learning Error - Amplifier {}\".format(ampNumber + 1), fontsize = 16)\n",
    "    axis.set_xlabel(\"Train size\", fontsize = 14)\n",
    "    axis.set_ylabel(\"MSE\", fontsize = 14)\n",
    "    axis.legend(loc = \"best\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gainArray = []\n",
    "lossArray = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97120419 0.89005236 0.91623037 0.91884817 0.94240838] => 0.93 (+/- 0.05)\n",
      "[0.9908377  0.94764398 0.98036649 0.97774869 0.96073298] => 0.97 (+/- 0.03)\n",
      "\n",
      "[0.80628272 0.86649215 0.87827225 0.85994764 0.82198953] => 0.85 (+/- 0.06)\n",
      "[0.4882199  0.51308901 0.44764398 0.47513089 0.53403141] => 0.49 (+/- 0.06)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.80628272, 0.86649215, 0.87827225, 0.85994764, 0.82198953]),\n",
       " array([0.4882199 , 0.51308901, 0.44764398, 0.47513089, 0.53403141]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#models = [] \n",
    "initModels()\n",
    "trainModel(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (42,) but got array with shape (40,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0672910c6e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotDistribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-2bdeaec3c23b>\u001b[0m in \u001b[0;36mplotDistribution\u001b[0;34m(models)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrain_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/autokeras/tasks/structured_data.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/autokeras/auto_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         y = utils.predict_with_adaptive_batch_size(\n\u001b[1;32m    431\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_feed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;31m# `ins` is a function when a distribute strategy is used in Eager mode.  In\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# that case `is_dataset` is True.  The code branches that have requirements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36m_prepare_feed_values\u001b[0;34m(model, inputs, targets, sample_weights, mode)\u001b[0m\n\u001b[1;32m    524\u001b[0m     inputs, targets, sample_weights = model._standardize_user_data(\n\u001b[1;32m    525\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         extract_tensors_from_dataset=True)\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2332\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2334\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2359\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2361\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Mestrado/ACOPResults/acopEnv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (42,) but got array with shape (40,)"
     ]
    }
   ],
   "source": [
    "yTest, predicted = plotDistribution(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotGainError(yTest, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotLossError(yTest, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLinkTestError(yTest, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = loadDataset()\n",
    "\n",
    "plotLearningCurves(0, models[0], X, y[:, 0: 2], 5)\n",
    "prediction = models[0].predict(X)\n",
    "\n",
    "for i in range(1, ampNumber):\n",
    "    newX = np.hstack((X, prediction))\n",
    "    \n",
    "    plotLearningCurves(i, models[i], X, y[:, i * 2 : i * 2 + 2], 5)\n",
    "    prediction = models[i].predict(newX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acopEnv",
   "language": "python",
   "name": "acopenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
