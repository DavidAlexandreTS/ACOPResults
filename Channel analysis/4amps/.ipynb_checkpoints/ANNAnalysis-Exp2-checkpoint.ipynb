{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import model_selection\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import pandas as pd\n",
    "\n",
    "layerArray       = [2, 3, 4, 5, 6]\n",
    "neuronArray      = [16, 32, 64, 128 ,256, 512, 1024]\n",
    "gainArray        = []\n",
    "lossArray        = []\n",
    "gainDistribution = []\n",
    "lossDistribution = []\n",
    "strategyArray    = []\n",
    "ampNumber        = 4\n",
    "\n",
    "originalDataSet = pd.read_csv('dataset.txt', sep = ',',header = None)\n",
    "binaryDataSet   = []\n",
    "\n",
    "for line in originalDataSet.values:\n",
    "    myList = [1 if i != 0 else 0 for i in line[:40]]\n",
    "    myList.extend(line[40:])\n",
    "    binaryDataSet.append(myList)\n",
    "\n",
    "binaryDataSet = pd.DataFrame(binaryDataSet)\n",
    "gainScaler    = MinMaxScaler(feature_range = (-1, 1))\n",
    "lossScaler    = MinMaxScaler(feature_range = (-1, 1))\n",
    "\n",
    "gainScalerAux = []\n",
    "lossScalerAux = []\n",
    "\n",
    "for i in range(0, ampNumber * 2, 2):\n",
    "    gainScalerAux.extend(binaryDataSet.values[:, 40 + i])\n",
    "    lossScalerAux.extend(binaryDataSet.values[:, 40 + i + 1])\n",
    "\n",
    "gainScaler.fit(np.array(gainScalerAux).reshape(-1, 1))\n",
    "lossScaler.fit(np.array(lossScalerAux).reshape(-1, 1))\n",
    "\n",
    "def loadDataset(): \n",
    "    dataSet = binaryDataSet.values[:, :40]\n",
    "    \n",
    "    for i in range(0, ampNumber * 2, 2):\n",
    "        gain    = np.array(binaryDataSet.values[:, 40 + i])\n",
    "        gain    = gainScaler.transform(gain.reshape(-1, 1))\n",
    "        dataSet = np.hstack((dataSet, gain))\n",
    "        \n",
    "        loss    = np.array(binaryDataSet.values[:, 40 + i + 1])\n",
    "        loss    = lossScaler.transform(gain.reshape(-1, 1))\n",
    "        dataSet = np.hstack((dataSet, loss))\n",
    "            \n",
    "    features, result = np.array(dataSet[:, :40]), np.array(dataSet[:, 40:])\n",
    "    return features, result\n",
    "\n",
    "def invertGainNorm(value):\n",
    "    auxArray = np.array([value, 0, 0, 0, 0, 0]).reshape(-1, 1)\n",
    "    return gainScaler.inverse_transform(auxArray)[0][0]\n",
    "\n",
    "def invertLossNorm(value):\n",
    "    auxArray = np.array([value, 0, 0, 0, 0, 0]).reshape(-1, 1)\n",
    "    return lossScaler.inverse_transform(auxArray)[0][0]\n",
    "    \n",
    "def getGainError(value1, value2):\n",
    "    return abs(invertGainNorm(value1) - invertGainNorm(value2))\n",
    "\n",
    "def getLossError(value1, value2):\n",
    "    return abs(invertLossNorm(value1) - invertLossNorm(value2))\n",
    "\n",
    "features, result = loadDataset()\n",
    "\n",
    "originalDataSet = pd.read_csv('dataset.txt', sep = ',',header = None)\n",
    "binaryDataSet   = []\n",
    "\n",
    "for line in originalDataSet.values:\n",
    "    myList = [1 if i != 0 else 0 for i in line[:40]]\n",
    "    myList.extend(line[40:])\n",
    "    binaryDataSet.append(myList)\n",
    "\n",
    "binaryDataSet = pd.DataFrame(binaryDataSet)\n",
    "gainScaler    = MinMaxScaler(feature_range = (-1, 1))\n",
    "lossScaler    = MinMaxScaler(feature_range = (-1, 1))\n",
    "\n",
    "gainScalerAux = []\n",
    "lossScalerAux = []\n",
    "\n",
    "for i in range(0, ampNumber * 2, 2):\n",
    "    gainScalerAux.extend(binaryDataSet.values[:, 40 + i])\n",
    "    lossScalerAux.extend(binaryDataSet.values[:, 40 + i + 1])\n",
    "\n",
    "gainScaler.fit(np.array(gainScalerAux).reshape(-1, 1))\n",
    "lossScaler.fit(np.array(lossScalerAux).reshape(-1, 1))\n",
    "\n",
    "def loadDataset(): \n",
    "    dataSet = binaryDataSet.values[:, :40]\n",
    "    \n",
    "    for i in range(0, ampNumber * 2, 2):\n",
    "        gain    = np.array(binaryDataSet.values[:, 40 + i])\n",
    "        gain    = gainScaler.transform(gain.reshape(-1, 1))\n",
    "        dataSet = np.hstack((dataSet, gain))\n",
    "        \n",
    "        loss    = np.array(binaryDataSet.values[:, 40 + i + 1])\n",
    "        loss    = lossScaler.transform(loss.reshape(-1, 1))\n",
    "        dataSet = np.hstack((dataSet, loss))\n",
    "    \n",
    "    features, result = np.array(dataSet[:, :40]), np.array(dataSet[:, 40:])\n",
    "    return features, result\n",
    "\n",
    "def invertGainNorm(value):\n",
    "    auxArray = np.array([value, 0, 0, 0, 0, 0]).reshape(-1, 1)\n",
    "    return gainScaler.inverse_transform(auxArray)[0][0]\n",
    "\n",
    "def invertLossNorm(value):\n",
    "    auxArray = np.array([value, 0, 0, 0, 0, 0]).reshape(-1, 1)\n",
    "    return lossScaler.inverse_transform(auxArray)[0][0]\n",
    "    \n",
    "def getGainError(value1, value2):\n",
    "    return abs(invertGainNorm(value1) - invertGainNorm(value2))\n",
    "\n",
    "def getLossError(value1, value2):\n",
    "    return abs(invertLossNorm(value1) - invertLossNorm(value2))\n",
    "\n",
    "features, result = loadDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(regressor, layers, features, result, folds = 5):\n",
    "    foldSize   = math.ceil(features.shape[0] / folds)\n",
    "    gainErrors = []\n",
    "    lossErrors = []\n",
    "    \n",
    "    for i in range(folds): \n",
    "        sliceBegin = i * foldSize\n",
    "        sliceEnd   = (i + 1) * foldSize\n",
    "        \n",
    "        X_train = np.delete(features, np.s_[sliceBegin: sliceEnd], 0)\n",
    "        y_train = np.delete(result, np.s_[sliceBegin: sliceEnd], 0)\n",
    "        \n",
    "        regressor.fit(X_train, y_train)     \n",
    "        \n",
    "        X_test = features[sliceBegin: sliceEnd]\n",
    "        y_test = result[sliceBegin: sliceEnd]\n",
    "        \n",
    "        gainError = 0\n",
    "        lossError = 0\n",
    "        \n",
    "        prediction = regressor.predict(X_test)\n",
    "        \n",
    "        for predicted, expected in zip(prediction, y_test):\n",
    "            for i in range(0, ampNumber * 2, 2):\n",
    "                gainError += getGainError(predicted[i], expected[i]) \n",
    "                lossError += getLossError(predicted[i + 1], expected[i + 1])\n",
    "                 \n",
    "        gainErrors.append((gainError / ampNumber) / foldSize)\n",
    "        lossErrors.append((lossError / ampNumber) / foldSize) # average loss error by amp\n",
    "\n",
    "    return np.array(gainErrors), np.array(lossErrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def trainModel(strategy, layers):    \n",
    "    features, result       = loadDataset()\n",
    "    gainErrors, lossErrors = crossValidate(strategy, layers, features, result)\n",
    "\n",
    "    print(gainErrors, \"=> %0.2f (+/- %0.2f)\" % (np.mean(gainErrors), gainErrors.std() * 2))\n",
    "    print(lossErrors, \"=> %0.2f (+/- %0.2f)\" % (np.mean(lossErrors), lossErrors.std() * 2))\n",
    "    print()\n",
    "    \n",
    "    gainArray.append(np.mean(gainErrors))\n",
    "    lossArray.append(np.mean(lossErrors))\n",
    "\n",
    "def setMLP(layers, neuronNumber):\n",
    "    return MLPRegressor(hidden_layer_sizes = [neuronNumber] * layers, activation = 'identity', learning_rate = 'constant', alpha = 0.001, random_state = 0, verbose = False)\n",
    "\n",
    "def getLoss(strategy):\n",
    "    features, result = loadDataset()\n",
    "    \n",
    "    trainSizes, trainScores, testScores = model_selection.learning_curve(strategy, \n",
    "                                                                   features, \n",
    "                                                                   result,\n",
    "                                                                   cv = 5,\n",
    "                                                                   train_sizes = [math.ceil(features.shape[0] / 5)],\n",
    "                                                                   scoring = 'neg_mean_squared_error')\n",
    "    \n",
    "    trainScores = [x * -1 for x in trainScores]\n",
    "    testScores = [x * -1 for x in testScores]\n",
    "    \n",
    "    return np.sqrt(trainScores), np.sqrt(testScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGainError():\n",
    "    fig7, axis = plt.subplots(figsize = (10, 5))\n",
    "\n",
    "    axis.plot(layerArray, gainArray, 'sb-')\n",
    "    axis.set_title(\"Gain error\")\n",
    "    axis.set_ylabel(\"Absolute error\", fontsize = 14)\n",
    "    axis.set_xlabel(\"Number of layers\", fontsize = 14)\n",
    "    #axis.legend(strategyArray)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLossError():\n",
    "    fig7, axis = plt.subplots(figsize = (10, 5))\n",
    "\n",
    "    axis.plot(layerArray, lossArray, 'or--')\n",
    "    axis.set_title(\"Loss error\")\n",
    "    axis.set_ylabel(\"Absolute error\", fontsize = 14)\n",
    "    axis.set_xlabel(\"Number of layers\", fontsize = 14)\n",
    "    #axis.legend(strategyArray)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTrainError(trainErrors):\n",
    "    fig, axis = plt.subplots(figsize = (10, 5))\n",
    "    \n",
    "    for i in range(len(layerArray)):\n",
    "        axis.plot(neuronArray, trainErrors)\n",
    "    \n",
    "    axis.set_title(\"Training Error\")\n",
    "    axis.set_ylabel(\"RMSE (dB)\", fontsize = 14)\n",
    "    axis.set_xlabel(\"Number of neurons in each hidden layer\", fontsize = 14)\n",
    "    axis.legend([\"{} layers\".format(x) for x in layerArray])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 layers - 16 neurons\n",
      "2 layers - 32 neurons\n",
      "2 layers - 64 neurons\n",
      "2 layers - 128 neurons\n",
      "2 layers - 256 neurons\n",
      "2 layers - 512 neurons\n",
      "2 layers - 1024 neurons\n",
      "[[0.3879297813838321, 0.37591790131816916, 0.3751638825357785, 0.37501258444599417, 0.37493941857411406, 0.37774548534560787, 0.37741250091151224]]\n",
      "3 layers - 16 neurons\n",
      "3 layers - 32 neurons\n",
      "3 layers - 64 neurons\n",
      "3 layers - 128 neurons\n",
      "3 layers - 256 neurons\n",
      "3 layers - 512 neurons\n",
      "3 layers - 1024 neurons\n",
      "[[0.3879297813838321, 0.37591790131816916, 0.3751638825357785, 0.37501258444599417, 0.37493941857411406, 0.37774548534560787, 0.37741250091151224], [0.38163162783165383, 0.37564412410320114, 0.37478560000412103, 0.37652434534827445, 0.3754567975073169, 0.3768965776351997, 0.37787036282617015]]\n",
      "4 layers - 16 neurons\n",
      "4 layers - 32 neurons\n",
      "4 layers - 64 neurons\n",
      "4 layers - 128 neurons\n",
      "4 layers - 256 neurons\n",
      "4 layers - 512 neurons\n",
      "4 layers - 1024 neurons\n",
      "[[0.3879297813838321, 0.37591790131816916, 0.3751638825357785, 0.37501258444599417, 0.37493941857411406, 0.37774548534560787, 0.37741250091151224], [0.38163162783165383, 0.37564412410320114, 0.37478560000412103, 0.37652434534827445, 0.3754567975073169, 0.3768965776351997, 0.37787036282617015], [0.3772044851572689, 0.3751843924767432, 0.37501399383975254, 0.37475904268239885, 0.3760995563514201, 0.3757411246539493, 0.37761272830576254]]\n",
      "5 layers - 16 neurons\n",
      "5 layers - 32 neurons\n",
      "5 layers - 64 neurons\n",
      "5 layers - 128 neurons\n",
      "5 layers - 256 neurons\n",
      "5 layers - 512 neurons\n",
      "5 layers - 1024 neurons\n",
      "[[0.3879297813838321, 0.37591790131816916, 0.3751638825357785, 0.37501258444599417, 0.37493941857411406, 0.37774548534560787, 0.37741250091151224], [0.38163162783165383, 0.37564412410320114, 0.37478560000412103, 0.37652434534827445, 0.3754567975073169, 0.3768965776351997, 0.37787036282617015], [0.3772044851572689, 0.3751843924767432, 0.37501399383975254, 0.37475904268239885, 0.3760995563514201, 0.3757411246539493, 0.37761272830576254], [0.37910066910040907, 0.3746352399578204, 0.374741398004422, 0.37527464952839124, 0.3745234242708412, 0.3753242770018595, 0.37601125249168976]]\n",
      "6 layers - 16 neurons\n",
      "6 layers - 32 neurons\n",
      "6 layers - 64 neurons\n",
      "6 layers - 128 neurons\n",
      "6 layers - 256 neurons\n",
      "6 layers - 512 neurons\n",
      "6 layers - 1024 neurons\n",
      "[[0.3879297813838321, 0.37591790131816916, 0.3751638825357785, 0.37501258444599417, 0.37493941857411406, 0.37774548534560787, 0.37741250091151224], [0.38163162783165383, 0.37564412410320114, 0.37478560000412103, 0.37652434534827445, 0.3754567975073169, 0.3768965776351997, 0.37787036282617015], [0.3772044851572689, 0.3751843924767432, 0.37501399383975254, 0.37475904268239885, 0.3760995563514201, 0.3757411246539493, 0.37761272830576254], [0.37910066910040907, 0.3746352399578204, 0.374741398004422, 0.37527464952839124, 0.3745234242708412, 0.3753242770018595, 0.37601125249168976], [0.37980435906588106, 0.37527009828201396, 0.37501334745603915, 0.37615114589484167, 0.3752356135450222, 0.37803309483705794, 0.37525853551988353]]\n"
     ]
    }
   ],
   "source": [
    "trainErrors = []\n",
    "\n",
    "for layerNumber in layerArray:\n",
    "    aux = []\n",
    "    for neuronNumber in neuronArray:\n",
    "        print(\"{} layers - {} neurons\".format(layerNumber, neuronNumber))\n",
    "        \n",
    "        regressor             = setMLP(layerNumber, neuronNumber)\n",
    "        trainError, testError = getLoss(regressor)\n",
    "\n",
    "        aux.append(np.mean(trainError[0]))\n",
    "    trainErrors.append(aux)\n",
    "print(trainErrors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotTrainError(trainErrors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
